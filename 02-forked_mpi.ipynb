{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forking out command line MPI tasks\n",
    "If we want to fork out MPI processes on the nodes, we can do that via the use of `mpi_wrap()` and then Dask manages the srun task that launches the job. `mpi_wrap()` returns a dictionary:\n",
    "```python\n",
    "{\"cmd\": cmd_launched, \"out\": stdout_output, \"err\": stderr_output}\n",
    "```\n",
    "This (currently) requires a little boilerplate code to work as one might expect (see the example below).\n",
    "## Interacting with the task\n",
    "If you need to grab information from the executed task, you can either do something to parse this dictionary, or interact with the executed task via the file system (i.e., read a result file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from jobqueue_features.clusters import CustomSLURMCluster\n",
    "from jobqueue_features.decorators import on_cluster, mpi_task\n",
    "from jobqueue_features.mpi_wrapper import mpi_wrap\n",
    "from jobqueue_features.functions import set_default_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_cluster(CustomSLURMCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cluster = CustomSLURMCluster(\n",
    "    name=\"mpiCluster\", walltime=\"00:04:00\", nodes=2, mpi_mode=True, fork_mpi=True, queue='devel'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need boilerplate code because our decorators insert some kwargs (that define things like the number of processes) that are required for `mpi_wrap` to execute the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@on_cluster(cluster=custom_cluster, cluster_id=custom_cluster.name)\n",
    "@mpi_task(cluster_id=custom_cluster.name)\n",
    "def mpi_wrap_task(**kwargs):\n",
    "    return mpi_wrap(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forked_mpi():\n",
    "    script_path = os.path.join(os.environ.get(\"JOBQUEUE_FEATURES_EXAMPLES\"), \"resources\", \"helloworld.py\")\n",
    "    t = mpi_wrap_task(executable=\"python\", exec_args=script_path)\n",
    "    print(\"Ran\\n\\t\", t.result()[\"cmd\"])\n",
    "    result = t.result()[\"out\"]\n",
    "    # Need to decode the output string so it is easily printed\n",
    "    return result.decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forked_mpi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobqueue",
   "language": "python",
   "name": "jobqueue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
