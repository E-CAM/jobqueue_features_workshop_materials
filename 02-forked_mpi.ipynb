{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forking out command line MPI tasks\n",
    "Imagine I have a workflow that requires me to run an MPI executable (e.g., `mpiexec -n 4 <my_executable>`) and process the result. We can do that via the use of the `mpi_wrap()` function from the library, such that Dask manages the MPI runtime task that launches the job.\n",
    "`mpi_wrap()` returns a dictionary:\n",
    "```python\n",
    "{\n",
    "    \"cmd\": cmd_launched, \n",
    "    \"out\": stdout_output, \n",
    "    \"err\": stderr_output\n",
    "}\n",
    "```\n",
    "This (currently) requires a little boilerplate code to work as one might expect (see the example below).\n",
    "\n",
    "## Interacting with the task\n",
    "If you need to grab information from the executed task, you can either do something to parse this dictionary, or interact with the executed task via the file system (e.g., read a result file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from jobqueue_features.clusters import CustomSLURMCluster\n",
    "from jobqueue_features.decorators import on_cluster, mpi_task\n",
    "from jobqueue_features.mpi_wrapper import mpi_wrap\n",
    "from jobqueue_features.clusters_controller import (\n",
    "    clusters_controller_singleton as controller,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time when we declare our cluster we need to add a few additional keyword arguments:\n",
    "* `mpi_mode` to let the cluster know we will use MPI\n",
    "* `fork_mpi`, to indicate that we will be forking MPI processes\n",
    "* `nodes` to indicate how much resources we want use (there are many other options for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cluster = CustomSLURMCluster(\n",
    "    name=\"mpiCluster\",\n",
    "    mpi_mode=True,\n",
    "    fork_mpi=True,\n",
    "    nodes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need boilerplate code because our decorators insert some kwargs that are required for `mpi_wrap` to execute the task (things like the number of processes since these usually have to be communicated to the MPI runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@on_cluster(cluster=custom_cluster)\n",
    "@mpi_task(cluster=custom_cluster)\n",
    "def mpi_wrap_task(**kwargs):\n",
    "    return mpi_wrap(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can use our newly created function `mpi_wrap_task()` in the same way we would use `mpi_wrap()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forked_mpi():\n",
    "    script_path = os.path.join(\"resources\", \"helloworld.py\")\n",
    "    t = mpi_wrap_task(\n",
    "        executable=\"python\", \n",
    "        exec_args=script_path\n",
    "    )\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not let's run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forked_mpi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a future, let's get the response and see what we ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = result.result()\n",
    "print(\"Ran\\n\\t\", final_result[\"cmd\"])\n",
    "output = final_result[\"out\"]\n",
    "# Need to decode the output string so it is easily printed\n",
    "print(output.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up after ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller._close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a new cluster that has the capability to scale. Each worker will have one node, and since we know we can have a maximum of 2 workers running at the same time, we can also set our `maximum_jobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_fork_cluster = CustomSLURMCluster(\n",
    "    name=\"multi_fork_cluster\",\n",
    "    mpi_mode=True,\n",
    "    fork_mpi=True,\n",
    "    nodes=1,\n",
    "    maximum_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redefine our task to use the new cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@on_cluster(cluster=multi_fork_cluster)\n",
    "@mpi_task(cluster=multi_fork_cluster)\n",
    "def mpi_wrap_task(**kwargs):\n",
    "    return mpi_wrap(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a whole list of tasks to run on the cluster and check which nodes they run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "for x in range(100):\n",
    "    tasks.append(forked_mpi())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's count the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_count = 0\n",
    "c2_count = 0\n",
    "for job in tasks:\n",
    "    result = job.result()[\"out\"]\n",
    "    if 'c1'.encode() in result:\n",
    "        c1_count += 1\n",
    "    elif 'c2'.encode() in result:\n",
    "        c2_count += 1\n",
    "print(\"c1: {} \\nc2: {}\".format(c1_count, c2_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bookkeeping for your `future`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But look in the dashboard, there is still a worker running and all our results are still in memory. This is because the futures are still in the current context and the garbage collector doesn't know we are finished with them yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if that running job dies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue | grep batch | awk '{print $1}' | xargs -i scancel {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything gets recalculated (eventually)!\n",
    "\n",
    "If we know we are finished with our future let's release it once we have what we need.\n",
    "\n",
    "Let's also take the opportunity to leverage a feature of `distributed` for working with futures, the function `as_complete()` which returns futures in the order that they complete. Watch the dashboard this time around to see the different behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import as_completed\n",
    "\n",
    "tasks = []\n",
    "for x in range(100):\n",
    "    tasks.append(forked_mpi())\n",
    "\n",
    "c1_count = 0\n",
    "c2_count = 0\n",
    "for job in as_completed(tasks):\n",
    "    result = job.result()[\"out\"]\n",
    "    # Now we have what we need, cancel the future\n",
    "    job.cancel()\n",
    "    \n",
    "    if 'c1'.encode() in result:\n",
    "        c1_count += 1\n",
    "    elif 'c2'.encode() in result:\n",
    "        c2_count += 1\n",
    "print(\"c1: {} \\nc2: {}\".format(c1_count, c2_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up after ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Redefine `forked_mpi` as `forked_mpi_2` function so it will execute the `resources/square.py` script.\n",
    "2. Get the result, split lines, map to integers and sum numbers from outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forked_mpi_2():\n",
    "    # 1. define script path\n",
    "    # 2. make a task with `mpi_wrap_task`\n",
    "    # 3. return task\n",
    "    pass # remove it and insert code here\n",
    "\n",
    "task = forked_mpi_2()  # 4. get task\n",
    "# 5. get result from task,\n",
    "# 6. get value from result with key \"out\" and map it to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller._close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
